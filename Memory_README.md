# ğŸ§  Memory System in Bengali RAG Chatbot

This chatbot doesnâ€™t just retrieve answers â€” it remembers the conversation while also staying grounded in the textbook. It uses **two different kinds of memory** for two different jobs:  

---

## ğŸ”¹ Short-Term Memory: Conversation Buffer

- **Implementation:** `ConversationBufferMemory` from LangChain  
- **Purpose:** Keeps track of the **chat flow** in the current session.

### âœ… What it does
- Stores the last few interactions between the user and the chatbot.
- Makes the chatbot feel more â€œhumanâ€ by allowing **follow-up questions**.

**Example:**
User: à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤?
Bot: à§§à§« à¦¬à¦›à¦°
User: à¦¤à¦¾à¦° à¦¬à¦¾à¦¬à¦¾à¦° à¦¨à¦¾à¦® à¦•à§€?
Bot: à¦¶à¦®à§à¦­à§à¦¨à¦¾à¦¥ à¦¸à§‡à¦¨

The bot knows â€œà¦¤à¦¾à¦°â€ means à¦•à¦²à§à¦¯à¦¾à¦£à§€ because that context lives in the buffer.

### âœ… Why use a buffer?
- **Lightweight:** No heavy databases or complicated infrastructure.
- **Clean sessions:** When a chat session ends, the memory resets, avoiding old irrelevant data bleeding into new conversations.
- **Perfect for educational chatbots:** Teachers and students ask questions in quick bursts â€” a buffer memory handles that naturally.

### ğŸš€ Alternatives for Production Scaling
If the chatbot needs to handle **persistent conversations across sessions**:
- **ConversationBufferWindowMemory** â€“ Keeps *only the last N turns* for lighter context windows.
- **ConversationKGMemory** â€“ Stores facts as a knowledge graph (â€œà¦•à¦²à§à¦¯à¦¾à¦£à§€ â†’ à¦¬à¦¯à¦¼à¦¸ â†’ à§§à§«â€) for structured recall.
- **Redis / Postgres-backed Memory** â€“ Makes the chatbot â€œrememberâ€ users across sessions (useful for tutoring systems or long-term study plans).

---

## ğŸ”¹ Long-Term Memory: Vector Database

- **Implementation:** FAISS (Facebook AI Similarity Search)
- **Purpose:** Stores and retrieves **textbook knowledge** efficiently.

### âœ… How it works
1. Text from HSC Bangla 1st Paper PDF is processed through **OCR** and cleaned.
2. The cleaned text is broken into **chunks** (500 characters each, with 100-character overlaps).
3. Each chunk is converted into a **768-dimensional embedding** using the multilingual-e5-base model.
4. These embeddings are stored in FAISS â€” which acts like a **search engine for meaning**.

**When you ask a question:**
- The question is converted into an embedding.
- FAISS finds the top matching chunks instantly.
- The LLM only reads those chunks â†’ **guaranteeing grounded answers.**

### âœ… Why FAISS?
- âš¡ **Speed**: Handles thousands of chunks in milliseconds.
- ğŸ’¾ **Efficiency**: Stores large volumes of data without blowing up memory.
- ğŸ“ˆ **Scalability**: Can handle multiple textbooks and even multiple subjects later.

### ğŸš€ Alternatives for Production Scaling
- **Chroma** â€“ Easier to set up for small projects, good LangChain integration.
- **Weaviate / Pinecone** â€“ Cloud-based vector DBs for massive datasets.
- **Postgres + pgvector** â€“ A relational DB solution if your team wants full SQL control.

---

## ğŸŒŸ Why This Dual Memory Setup Works
âœ… **Short-term memory** = The bot can hold a conversation naturally, understand â€œà¦¤à¦¾à¦°â€ and â€œà¦“à¦Ÿà¦¾.â€  
âœ… **Long-term memory** = The bot always stays grounded in the HSC Bangla 1st Paper text.
